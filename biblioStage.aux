\relax 
\catcode `:\active 
\catcode `;\active 
\catcode `!\active 
\catcode `?\active 
\select@language{french}
\@writefile{toc}{\select@language{french}}
\@writefile{lof}{\select@language{french}}
\@writefile{lot}{\select@language{french}}
\citation{chatfield2014return}
\citation{bailly2015bag}
\citation{bailly2015bag}
\citation{chatfield2014return}
\citation{srivastava2014dropout}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}State of the art}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Bag-of-Words}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Support Machine Vector}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neurals Networks and Time Series}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Neural Network}{1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Le bag-of-words est en bas de la figure, il est compos\'e de diff\'erents mots, ici des images, qui sont reconnus sur les documents (en haut), qui sont ici des images. Par exemple, pour la premi\`ere image, on a une forte correspondance avec le premier et deuxi\`eme mot, alors que l’on a une faible correspondance avec la derni\`ere image.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:BoW}{{1}{2}}
\newlabel{fig:nnl}{{2a}{2}}
\newlabel{sub@fig:nnl}{{a}{2}}
\newlabel{fig:nnc}{{2b}{2}}
\newlabel{sub@fig:nnc}{{b}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Il existe diff\'erents types de r\'eseaux de neurones. Par exemple, la figure 1.a est un r\'eseau avec une architecture \`a couche, alors que le r\'eseau de la figure 2.b \`a une architecture \`a connexion compl\`ete. Dans le domaine de la classificatin, l'architecture \`a couche est souvent utilis\'e.\relax }}{2}}
\newlabel{fig:neuralNetwork}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces La fonction d\textquotesingle activation est la fonction qui va d\'eterminer le comportement du neurone. Par exemple, si la fonction d\textquotesingle activation est un threshold, la sortie sera soit 1, soit 0. Si c’est une sigmoide (le plus souvent utlis\'e), la sortie sera y = 1 / (1 + exp(-x)) avec y la sortie et x l\textquotesingle entr\'ee.\relax }}{3}}
\newlabel{fig:neural}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}CNN}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Ici, imaginons que la couche m-1 est une r\'etine o\`u chaque neurone correspond \`a un pixel d\textquotesingle une image. Dans la couche m, on s\textquotesingle aper\c coit que chaque neurone a 3 entr\'ees qui correspondent chacune \`a un pixel. Chaque neurone de la couche m est donc un sous-ensemble de l\textquotesingle image de 3 pixels. De la m\^eme fa\c con, chaque neurone de la couche m+1 est donc un sous-ensemble de l\textquotesingle image de 5 pixels.\relax }}{3}}
\newlabel{fig:cnn}{{4}{3}}
\citation{zheng2014time}
\citation{chatfield2014return}
\citation{krizhevsky2012imagenet}
\citation{howard2013some}
\citation{howard2013some}
\citation{krizhevsky2012imagenet}
\citation{krizhevsky2012imagenet}
\citation{howard2013some}
\citation{zheng2014time}
\citation{jia2014caffe}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Time Series}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Data-augmentation for Time Series}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pourquoi chercher \`a augmenter la base de donn\'es ?}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Comment y parvenir ?}{4}}
\citation{*}
\bibstyle{plain}
\bibdata{biblio}
\bibcite{bailly2015bag}{1}
\bibcite{chatfield2014return}{2}
\bibcite{howard2013some}{3}
\bibcite{jia2014caffe}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Exemple d\textquotesingle augmentation de donn\'ee pour une image. I\c ci, on a obtenu six images \`a partie d\textquotesingle une seule. On a d\textquotesingle abord appliqu\'e une inversion d\textquotesingle image et puis on a rogn\'e les images obtenues de deux mani\`eres diff\'erentes. En r\'ealit\'e, beaucoup plus de transformations sont appliqu\'ees, permettant de multiplier la base de donn\'ees des images de beaucoup (1024 dans~\cite  {howard2013some}).\relax }}{5}}
\newlabel{fig:dataAugmentationChat}{{5}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusion}{5}}
\bibcite{krizhevsky2012imagenet}{5}
\bibcite{srivastava2014dropout}{6}
\bibcite{zheng2014time}{7}
